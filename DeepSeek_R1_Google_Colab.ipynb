{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "colab": {
      "name": "DeepSeek-R1_with_Ollama.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek-R1 on Google Colab with Ollama\n",
        "\n",
        "Welcome! This notebook will show you how to:\n",
        "1. Install **Ollama** on Google Colab.\n",
        "2. Download the **DeepSeek-R1:7B** model.\n",
        "3. Run some test prompts.\n",
        "4. Create a small \"chat\" loop.\n",
        "5. Measure the speed (tokens per second).\n",
        "\n",
        "The instructions are written in simple English. Please follow them step by step.\n",
        "\n",
        "> **Important**: Ollama has experimental support on Linux (and thus on Colab). Some things might not work smoothly. We'll do our best!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check if we have a GPU (Optional)\n",
        "\n",
        "Google Colab often provides a free GPU. If you have a GPU, this can make model inference faster.\n",
        "\n",
        "You can check if a GPU is available by running the following cell. If it says something like \"Tesla T4\" or \"Tesla P100\" or \"Tesla V100\" etc., then you have a GPU!\n",
        "\n",
        "If it shows only CPU, you can still run the model, but it may be slower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "# This command shows if a GPU is connected.\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Ollama\n",
        "\n",
        "We will use a shell script provided by Ollama to install their software on Linux (the operating system in Colab). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "# Install Ollama\n",
        "# The following command downloads and executes the install script from the official website.\n",
        "# This might take a bit of time to run.\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Pull (download) the DeepSeek-R1:7B model\n",
        "\n",
        "We will now download the DeepSeek-R1:7B model. This may take some time depending on the file size and your internet connection.\n",
        "\n",
        "> **Note**: If this step fails, it might be because:\n",
        "1. The Colab session ran out of RAM or disk space.\n",
        "2. Network issues.\n",
        "3. The model is very large.\n",
        "\n",
        "You can check your available Colab disk space by using the command `!df -h` if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "# Download DeepSeek-R1 7B model\n",
        "!ollama pull deepseek-r1:7b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test a Simple Prompt\n",
        "\n",
        "We will run a simple test prompt using Ollama. We'll send a quick message to see if the model responds.\n",
        "\n",
        "### Explanation of the command:\n",
        "`ollama run deepseek-r1:7b --prompt \"Hello!\"`\n",
        "- **ollama run**: The command to run a model.\n",
        "- **deepseek-r1:7b**: The name of the model we want.\n",
        "- **--prompt \"Hello!\"**: The text we send to the model.\n",
        "\n",
        "Give it a try below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "# Let's try a simple prompt\n",
        "!ollama run deepseek-r1:7b --prompt \"Hello there! How are you today?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create a Chat Loop\n",
        "\n",
        "Sometimes we want to chat with the model more than once and keep track of the conversation. One way is to:\n",
        "1. Keep track of all user messages.\n",
        "2. Keep track of all model responses.\n",
        "3. Put them all into a single prompt each time.\n",
        "4. Pass that combined text to the model.\n",
        "\n",
        "Below is a simple Python loop that:\n",
        "1. Asks you for input.\n",
        "2. Builds a conversation string.\n",
        "3. Calls `ollama run` with that conversation.\n",
        "4. Prints out the model's response.\n",
        "5. Repeats until you type `exit`.\n",
        "\n",
        "> **Note**: This is a basic chat. It doesn't store conversation in a robust way, and each turn might be somewhat slow. If you want a more advanced chat, you may need to use `ollama serve` and then call the Ollama API from Python. But for now, this is good enough to see how the model responds to multiple prompts in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "conversation = []  # List to store the conversation\n",
        "\n",
        "print(\"Start chatting with DeepSeek-R1. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    \n",
        "    # Add user message to conversation\n",
        "    conversation.append(\"User:\" + user_input)\n",
        "\n",
        "    # Combine conversation into a single prompt\n",
        "    # We'll put each turn on its own line.\n",
        "    prompt_text = \"\\n\".join(conversation) + \"\\nAssistant:\"\n",
        "\n",
        "    # Build the command for Ollama\n",
        "    # We'll pass the combined conversation as the prompt.\n",
        "    cmd = f\"ollama run deepseek-r1:7b --prompt {shlex.quote(prompt_text)}\"\n",
        "\n",
        "    # Run the command and capture output\n",
        "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "\n",
        "    # Decode response\n",
        "    response = stdout.decode('utf-8')\n",
        "\n",
        "    # Print model response\n",
        "    print(\"Model:\", response.strip())\n",
        "\n",
        "    # Add Assistant's response to conversation\n",
        "    conversation.append(\"Assistant:\" + response.strip())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Inference Speed (Tokens per Second)\n",
        "\n",
        "We can do a quick test to see how fast the model generates tokens. This won't be perfect, but it can give us an idea.\n",
        "\n",
        "**Method**: We will\n",
        "1. Use the `time` magic in Colab (`%%time`) to measure execution time.\n",
        "2. Prompt the model with a short message.\n",
        "3. Then estimate how many tokens were produced.\n",
        "\n",
        "Ollama's output in the terminal doesn't always show the token count. If you need to track it exactly, you might need advanced logging or the `--metadata` flag. For now, let's just measure how long it takes to get a short response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "source": [
        "# We'll time a single run with a short prompt.\n",
        "%%time\n",
        "!ollama run deepseek-r1:7b --prompt \"Explain the theory of gravity in 1-2 sentences, like I'm 12 years old.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the output above. You will see how many seconds the command took. If the model responded with around 50 tokens in 3 seconds, that's ~16 tokens/second. This is just a rough measurement.\n",
        "\n",
        "### Tip\n",
        "If you want more precise speed data, you can parse logs from Ollama or add advanced flags. Also, speed often depends on your Colab instance, GPU availability, the number of threads, and other factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Done!\n",
        "\n",
        "You have successfully installed Ollama, downloaded the DeepSeek-R1:7B model, tested prompts, chatted, and checked speed. Feel free to modify the notebook or add your own prompts.\n",
        "\n",
        "Remember, Google Colab sessions can expire. If it expires, you'll need to rerun everything (including the download step). If you plan to do serious work, you might want to pay for more storage or more stable sessions."
      ]
    }
  ]
}
